{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746ed65d-9331-40af-883c-b0302584665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6f6c84-f4dc-45a3-9fa8-f1c829ad7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data #####\n",
    "data = \"나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c452481c-bded-43e2-a689-6037bb9ccc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 preprocessing 해주는 부분입니다\n",
    "#RNN과 동일한 방법입니다\n",
    "def data_preprocessing(data):\n",
    "    data = re.sub('[^가-힣]', ' ', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    Word_to_ix = {Word: i for i, Word in enumerate(vocab)}\n",
    "    ix_to_Word = {i: Word for i, Word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, Word_to_ix, ix_to_Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8388c374-ec27-4ad7-b79d-8d666cbcf65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수\n",
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def sigmoid_derivative(input):\n",
    "    return input * (1 - input)\n",
    "    \n",
    "def tanh(input, derivative=False):\n",
    "    return np.tanh(input)\n",
    "\n",
    "def tanh_derivative(input):\n",
    "    return 1 - input ** 2\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.sum(np.exp(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56753766-f611-489b-8882-0b79b57b67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate\n",
    "        self.Wf = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.Wi = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.Wc = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.Wo = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.Wy = np.random.randn(output_size, hidden_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    # 네트워크 메모리 리셋\n",
    "    def reset(self):\n",
    "        self.X = {}\n",
    "\n",
    "        self.HS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.CS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.C = {}\n",
    "        self.O = {}\n",
    "        self.F = {}\n",
    "        self.I = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward 순전파\n",
    "    def forward(self, inputs):\n",
    "        # self.reset()\n",
    "        x = {}\n",
    "        outputs = []\n",
    "        for t in range(len(inputs)):\n",
    "            x[t] = np.zeros((vocab_size , 1))\n",
    "            x[t][inputs[t]] = 1  # 각각의 Word에 대한 one hot coding\n",
    "            self.X[t] = np.concatenate((self.HS[t - 1], x[t]))\n",
    "\n",
    "            self.F[t] = sigmoid(np.dot(self.Wf, self.X[t]) + self.bf)\n",
    "            self.I[t] = sigmoid(np.dot(self.Wi, self.X[t]) + self.bi)\n",
    "            self.C[t] = tanh(np.dot(self.Wc, self.X[t]) + self.bc)\n",
    "            self.O[t] = sigmoid(np.dot(self.Wo, self.X[t]) + self.bo)\n",
    "\n",
    "            self.CS[t] = self.F[t] * self.CS[t - 1] + self.I[t] * self.C[t]\n",
    "            self.HS[t] = self.O[t] * tanh(self.CS[t])\n",
    "\n",
    "            outputs += [np.dot(self.Wy, self.HS[t]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # 역전파\n",
    "    def backward(self, errors, inputs):\n",
    "        dLdWf, dLdbf = 0, 0\n",
    "        dLdWi, dLdbi = 0, 0\n",
    "        dLdWc, dLdbc = 0, 0\n",
    "        dLdWo, dLdbo = 0, 0\n",
    "        dLdWy, dLdby = 0, 0\n",
    "\n",
    "        dh_next, dc_next = np.zeros_like(self.HS[0]), np.zeros_like(self.CS[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            error = errors[t]\n",
    "\n",
    "            # Final Gate Weights and Biases Errors\n",
    "            dLdWy += np.dot(error, self.HS[t].T)         #𝜕𝐿/𝜕𝑊𝑦\n",
    "            dLdby += error                               #𝜕𝐿/𝜕b𝑦 = (𝜕𝐿/𝜕z_t)(𝜕z_t/𝜕b𝑦) = error x 1 (Zt = WyHSt + by)\n",
    "            \n",
    "            # Hidden State Error\n",
    "            dLdHS = np.dot(self.Wy.T, error) + dh_next    #𝜕𝐿/𝜕𝐻𝑆\n",
    "\n",
    "            # Output Gate Weights and Biases Errors\n",
    "            dLdo = tanh(self.CS[t]) * dLdHS * sigmoid_derivative(self.O[t])\n",
    "            dLdWo += np.dot(dLdo, inputs[t].T)\n",
    "            dLdbo += dLdo\n",
    "\n",
    "            # Cell State Error\n",
    "            dLdCS = tanh_derivative(tanh(self.CS[t])) * self.O[t] * dLdHS + dc_next\n",
    "\n",
    "            # Forget Gate Weights and Biases Errors\n",
    "            dLdf = dLdCS * self.CS[t - 1] * sigmoid_derivative(self.F[t])\n",
    "            dLdWf += np.dot(dLdf, inputs[t].T)\n",
    "            dLdbf += dLdf\n",
    "\n",
    "            # Input Gate Weights and Biases Errors\n",
    "            dLdi = dLdCS * self.C[t] * sigmoid_derivative(self.I[t])\n",
    "            dLdWi += np.dot(dLdi, inputs[t].T)\n",
    "            dLdbi += dLdi\n",
    "\n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            dLdc = dLdCS * self.I[t] * tanh_derivative(self.C[t])\n",
    "            dLdWc += np.dot(dLdc, inputs[t].T)\n",
    "            dLdbc += dLdc\n",
    "\n",
    "            # Concatenated Input Error (Sum of Error at Each Gate!)\n",
    "            d_z = np.dot(self.Wf.T, dLdf) + np.dot(self.Wi.T, dLdi) + np.dot(self.Wc.T, dLdc) + np.dot(self.Wo.T, dLdo)\n",
    "\n",
    "            # Error of Hidden State and Cell State at Next Time Step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.F[t] * dLdCS\n",
    "            \n",
    "        for d_ in (dLdWf, dLdbf, dLdWi, dLdbi, dLdWc, dLdbc, dLdWo, dLdbo, dLdWy, dLdby):\n",
    "            np.clip(d_, -1, 1, out=d_)\n",
    "\n",
    "        self.Wf += dLdWf * self.learning_rate * (-1)\n",
    "        self.bf += dLdbf * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wi += dLdWi * self.learning_rate * (-1)\n",
    "        self.bi += dLdbi * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wc += dLdWc * self.learning_rate * (-1)\n",
    "        self.bc += dLdbc * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wo += dLdWo * self.learning_rate * (-1)\n",
    "        self.bo += dLdbo * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wy += dLdWy * self.learning_rate * (-1)\n",
    "        self.by += dLdby * self.learning_rate * (-1)\n",
    "\n",
    "    # Train\n",
    "    def train(self, inputs, labels):\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            self.reset()\n",
    "            input_idx = [Word_to_ix[input] for input in inputs]\n",
    "            predictions = self.forward(input_idx)\n",
    "\n",
    "            errors = []\n",
    "            for t in range(len(predictions)):\n",
    "                errors += [softmax(predictions[t])]\n",
    "                errors[-1][Word_to_ix[labels[t]]] -= 1\n",
    "\n",
    "            self.backward(errors, self.X)\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([Word_to_ix[input] for input in inputs])\n",
    "\n",
    "        gt = ''\n",
    "        output = '나라의 '\n",
    "        for q in range(len(labels)):\n",
    "            prediction = ix_to_Word[np.argmax(softmax(probabilities[q].reshape(-1)))]\n",
    "            gt += inputs[q] + ' '\n",
    "            output += prediction + ' '\n",
    "            \n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "\n",
    "        print('실제값: ', gt)\n",
    "        print('예측값: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6edb6f-28b6-466a-9977-a8fadfd2fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 371.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제값:  나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 \n",
      "예측값:  나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 25\n",
    "\n",
    "# data preparation\n",
    "tokens, vocab_size, Word_to_ix, ix_to_Word = data_preprocessing(data)\n",
    "train_X, train_y = tokens[:-1], tokens[1:]\n",
    "\n",
    "lstm = LSTM(input_size=vocab_size + hidden_size, hidden_size=hidden_size, output_size=vocab_size, num_epochs=1000,\n",
    "            learning_rate=0.05)\n",
    "\n",
    "##### Training #####\n",
    "lstm.train(train_X, train_y)\n",
    "\n",
    "lstm.test(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c599bf-92f5-4c3c-9042-06c7843229fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_env",
   "language": "python",
   "name": "fusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
