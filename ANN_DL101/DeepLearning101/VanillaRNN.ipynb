{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a988e99e-2bd0-489c-8208-975ccc3a7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ffa1fa-f40b-4f2b-9d6c-a973d62d4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f17b06-3d6b-46dd-a8a9-58d5ae062c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    data = re.sub('[^가-힣]', ' ', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "    ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_ix, ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67870ae5-5fc1-4785-ad1d-c345a639d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U,W,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fe7361-bea7-40cb-bd1c-b1693bbab8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, targets, hprev):\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1  # 각각의 word에 대한 one hot coding \n",
    "        hs[i] = np.tanh(np.dot(U, xs[i]) + np.dot(W, hs[i - 1]))\n",
    "        ys[i] = np.dot(V, hs[i])\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i]))  # softmax계산\n",
    "        loss += -np.log(ps[i][targets[i], 0])\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3930ee7e-bc5a-49ab-b578-9e9303fcf94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "\n",
    "    # Backward propagation through time (BPTT)\n",
    "    # 처음에 모든 가중치들은 0으로 설정\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "        # 매번 i스텝에서 dL/dVi를 구하기\n",
    "        dV_step_i = ps[i] @ (hs[i]).T  # (y_hat - y) @ hs.T - for each step\n",
    "\n",
    "        dV = dV + dV_step_i  # dL/dVi를 다 더하기\n",
    "\n",
    "        # 각i별로 V와 W를 구하기 위해서는\n",
    "        # 먼저 공통적으로 계산되는 부분을 delta로 해서 계산해두고\n",
    "        # 그리고 시간을 거슬러 dL/dWij와 dL/dUij를 구한 뒤\n",
    "        # 각각을 합하여 dL/dW와 dL/dU를 구하고 \n",
    "        # 다시 공통적으로 계산되는 delta를 업데이트\n",
    "\n",
    "        # i번째 스텝에서 공통적으로 사용될 delta\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2)\n",
    "\n",
    "        # 시간을 거슬러 올라가서 dL/dW와 dL/dU를 구하\n",
    "        for j in range(i + 1)[::-1]:\n",
    "            dW_ij = delta_recent @ hs[j - 1].T\n",
    "\n",
    "            dW = dW + dW_ij\n",
    "\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU = dU + dU_ij\n",
    "\n",
    "            # 그리고 다음번 j번째 타임에서 공통적으로 계산할 delta를 업데이트\n",
    "            delta_recent = (W.T @ delta_recent) * (1 - hs[j - 1] ** 2)\n",
    "\n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "    return dU, dW, dV, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488c0f17-f587-4111-8910-430b6358259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_ix[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))    # 소프트맥스\n",
    "        ix = np.argmax(p)                    # 가장 높은 확률의 index를 리턴\n",
    "        x = np.zeros((vocab_size, 1))        # 다음번 input x를 준비\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix) \n",
    "    pred_words = ' '.join(ix_to_word[i] for i in ixes)\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8275ef0d-d4be-4c52-ae70-0148eded7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 parameters\n",
    "epochs = 10000\n",
    "h_size = 100\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592c4adf-bb3b-41e7-99cd-3fb7e14ee447",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab_size, word_to_ix, ix_to_word = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231d7074-2b8e-4bb3-8fef-41cc3dcbae57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '하고자',\n",
       " 1: '씀에',\n",
       " 2: '어리석은',\n",
       " 3: '바가',\n",
       " 4: '따름이니라',\n",
       " 5: '아니하기에',\n",
       " 6: '나라의',\n",
       " 7: '제',\n",
       " 8: '중국과',\n",
       " 9: '서로',\n",
       " 10: '많으니라',\n",
       " 11: '통하지',\n",
       " 12: '만드노니',\n",
       " 13: '이를',\n",
       " 14: '능히',\n",
       " 15: '글자를',\n",
       " 16: '내가',\n",
       " 17: '사람이',\n",
       " 18: '달라',\n",
       " 19: '뜻을',\n",
       " 20: '여겨',\n",
       " 21: '사람마다',\n",
       " 22: '마침내',\n",
       " 23: '이런',\n",
       " 24: '가엾이',\n",
       " 25: '못할',\n",
       " 26: '하여',\n",
       " 27: '할',\n",
       " 28: '스물여덟',\n",
       " 29: '까닭으로',\n",
       " 30: '날로',\n",
       " 31: '백성이',\n",
       " 32: '새로',\n",
       " 33: '문자와',\n",
       " 34: '이르고자',\n",
       " 35: '편안케',\n",
       " 36: '쉬이',\n",
       " 37: '말이',\n",
       " 38: '익혀',\n",
       " 39: '있어도',\n",
       " 40: '펴지',\n",
       " 41: '위해'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda76d7d-7c5e-4c6a-b46e-c0e8c278884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size) # 학습해야할 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8acdae2-f872-4e17-9188-855475e4cb80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 11.212595204699813\n",
      "epoch 100, loss: 1.9755354057074195\n",
      "epoch 200, loss: 0.2642002258370004\n",
      "epoch 300, loss: 0.13381121744400643\n",
      "epoch 400, loss: 0.08618063287090691\n",
      "epoch 500, loss: 0.06328666777984322\n",
      "epoch 600, loss: 0.0510343924522835\n",
      "epoch 700, loss: 0.04302657791085474\n",
      "epoch 800, loss: 0.06872060929125684\n",
      "epoch 900, loss: 0.041077596447223697\n",
      "epoch 1000, loss: 0.03031668642315866\n",
      "epoch 1100, loss: 0.02446655965506627\n",
      "epoch 1200, loss: 0.020800923526575726\n",
      "epoch 1300, loss: 0.018258231054130113\n",
      "epoch 1400, loss: 0.016354352319536945\n",
      "epoch 1500, loss: 0.014850894596336747\n",
      "epoch 1600, loss: 0.013623378075662669\n",
      "epoch 1700, loss: 0.012600278695647893\n",
      "epoch 1800, loss: 0.011734073157245789\n",
      "epoch 1900, loss: 0.010989193074415165\n",
      "epoch 2000, loss: 0.010338019954615245\n",
      "epoch 2100, loss: 0.009759564954741302\n",
      "epoch 2200, loss: 0.009238399268566025\n",
      "epoch 2300, loss: 0.00876344900221213\n",
      "epoch 2400, loss: 0.008326951536785377\n",
      "epoch 2500, loss: 0.00792362853761014\n",
      "epoch 2600, loss: 0.0075499685001419615\n",
      "epoch 2700, loss: 0.007203580881453335\n",
      "epoch 2800, loss: 0.006882662466527852\n",
      "epoch 2900, loss: 0.006585616002012483\n",
      "epoch 3000, loss: 0.006310819422247832\n",
      "epoch 3100, loss: 0.006056514467784637\n",
      "epoch 3200, loss: 0.0058207827646358686\n",
      "epoch 3300, loss: 0.0056015901548261355\n",
      "epoch 3400, loss: 0.005396884003685975\n",
      "epoch 3500, loss: 0.005204716826090491\n",
      "epoch 3600, loss: 0.005023357768373574\n",
      "epoch 3700, loss: 0.004851359595520062\n",
      "epoch 3800, loss: 0.004687572216998234\n",
      "epoch 3900, loss: 0.004531116546774055\n",
      "epoch 4000, loss: 0.0043813405222207485\n",
      "epoch 4100, loss: 0.00423777354338642\n",
      "epoch 4200, loss: 0.00410008601275989\n",
      "epoch 4300, loss: 0.0039680543134485245\n",
      "epoch 4400, loss: 0.003841529836487707\n",
      "epoch 4500, loss: 0.003720411495529286\n",
      "epoch 4600, loss: 0.003604622281145936\n",
      "epoch 4700, loss: 0.0034940907531813537\n",
      "epoch 4800, loss: 0.003388737966047638\n",
      "epoch 4900, loss: 0.0032884696163070284\n",
      "epoch 5000, loss: 0.0031931725941519214\n",
      "epoch 5100, loss: 0.0031027147957585583\n",
      "epoch 5200, loss: 0.00301694701478574\n",
      "epoch 5300, loss: 0.0029357058915926377\n",
      "epoch 5400, loss: 0.002858817155958541\n",
      "epoch 5500, loss: 0.0027860986734226136\n",
      "epoch 5600, loss: 0.0027173630474553135\n",
      "epoch 5700, loss: 0.0026524197157887196\n",
      "epoch 5800, loss: 0.00259107660259127\n",
      "epoch 5900, loss: 0.002533141452277337\n",
      "epoch 6000, loss: 0.0024784229851268613\n",
      "epoch 6100, loss: 0.002426731992402692\n",
      "epoch 6200, loss: 0.0023778824440564613\n",
      "epoch 6300, loss: 0.002331692629820168\n",
      "epoch 6400, loss: 0.00228798630744439\n",
      "epoch 6500, loss: 0.002246593799161063\n",
      "epoch 6600, loss: 0.002207352963966705\n",
      "epoch 6700, loss: 0.0021701099787721756\n",
      "epoch 6800, loss: 0.002134719881761579\n",
      "epoch 6900, loss: 0.002101046859849268\n",
      "epoch 7000, loss: 0.00206896429189004\n",
      "epoch 7100, loss: 0.0020383545845176904\n",
      "epoch 7200, loss: 0.0020091088546339998\n",
      "epoch 7300, loss: 0.0019811265205079524\n",
      "epoch 7400, loss: 0.001954314862759584\n",
      "epoch 7500, loss: 0.001928588608716353\n",
      "epoch 7600, loss: 0.0019038695799980716\n",
      "epoch 7700, loss: 0.0018800864248305865\n",
      "epoch 7800, loss: 0.0018571744340009075\n",
      "epoch 7900, loss: 0.0018350754140436677\n",
      "epoch 8000, loss: 0.0018137375657489019\n",
      "epoch 8100, loss: 0.001793115295645677\n",
      "epoch 8200, loss: 0.0017731688796427045\n",
      "epoch 8300, loss: 0.001753863908430033\n",
      "epoch 8400, loss: 0.001735170477228297\n",
      "epoch 8500, loss: 0.0017170621345344995\n",
      "epoch 8600, loss: 0.0016995146641074457\n",
      "epoch 8700, loss: 0.001682504823940438\n",
      "epoch 8800, loss: 0.0016660091885929299\n",
      "epoch 8900, loss: 0.0016500032282545105\n",
      "epoch 9000, loss: 0.0016344607135289524\n",
      "epoch 9100, loss: 0.0016193534744297026\n",
      "epoch 9200, loss: 0.0016046514854668587\n",
      "epoch 9300, loss: 0.0015903232115420135\n",
      "epoch 9400, loss: 0.0015763361371571721\n",
      "epoch 9500, loss: 0.0015626574091126022\n",
      "epoch 9600, loss: 0.0015492545388948076\n",
      "epoch 9700, loss: 0.0015360961237352669\n",
      "epoch 9800, loss: 0.0015231525488322126\n",
      "epoch 9900, loss: 0.0015103966285283659\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for p in range(len(tokens)-seq_len):\n",
    "        inputs = [word_to_ix[tok] for tok in tokens[p:p + seq_len]]\n",
    "        targets = [word_to_ix[tok] for tok in tokens[p + 1:p + seq_len + 1]]\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs)\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "\n",
    "        # p += seq_len\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4086039d-cfd8-4573-915a-a8d07dcb1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  나라의\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말이 중국과 달라 문자와 달라 문자와 서로 문자와 서로 통하지 서로 통하지 아니하기에 통하지 아니하기에 이런 아니하기에 이런 까닭으로 어리석은 까닭으로 어리석은 백성이 어리석은 백성이 이르고자 백성이 이르고자 할 이르고자 할 바가 있어도 바가 있어도 마침내 있어도 마침내 제 마침내\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  중국과\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "달라 문자와 달라 문자와 서로 문자와 서로 통하지 서로 통하지 아니하기에 통하지 아니하기에 이런 까닭으로 이런 까닭으로 어리석은 까닭으로 어리석은 백성이 어리석은 백성이 이르고자 백성이 이르고자 할 바가 할 바가 있어도 바가 있어도 마침내 있어도 마침내 제 마침내 제 뜻을\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  뜻을\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "능히 펴지 못할 마침내 제 마침내 제 뜻을 서로 문자와 서로 통하지 서로 통하지 아니하기에 통하지 아니하기에 이런 아니하기에 이런 까닭으로 어리석은 까닭으로 어리석은 백성이 어리석은 백성이 이르고자 백성이 이르고자 할 이르고자 할 바가 있어도 바가 있어도 마침내 있어도 마침내\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  마침내\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제 뜻을 제 마침내 제 마침내 제 뜻을 익혀 사람이 익혀 사람이 새로 스물여덟 글자를 스물여덟 글자를 스물여덟 글자를 만드노니 글자를 만드노니 사람마다 만드노니 사람마다 하여 사람마다 하여 쉬이 하여 쉬이 익혀 쉬이 익혀 날로 익혀 날로 씀에 날로 씀에\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  break\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    try:\n",
    "        user_input = input(\"input: \")\n",
    "        if user_input == 'break':\n",
    "            break\n",
    "        response = predict(user_input,40)\n",
    "        print(response)\n",
    "    except:\n",
    "        print('Uh oh try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3162cb-4ba1-416d-9f1c-003121bd0cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
